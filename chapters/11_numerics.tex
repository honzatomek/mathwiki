\newpage
\chapter{Numerics}

\section{Factorisation}

\subsection{Cholesky Factorisation}

Cholesky Factorisation is a matrix operation such that a matrix $ \m{A} $ is
decomposed to a multiplication of a lower triangular martrix $ \m{L} $ and its
conjugate transpose $ \m{L}^T $.

The \textbf{Cholesky Factorisation} is applicable to a \textbf{Hermitian},
\textbf{positive definite} matrices.

\begin{equation}
    \m{A} = \m{L} \m{L}^*
\end{equation}

Every Hermitian positive definite matrix (and thus every real-valued symmetric
positive definite matrix) has a unique Cholesky factorisation.

If $ \m{A} $ can be written as $ \m{L} \m{L}^* $ for some invertible $ \m{L} $,
lower triangular or otherwise, then $ \m{A} $ is Hermitian and positive definite.

When $ \m{A} $ is a real matrix (hence symmetric and positive definite), the
factorisation may be written:

\begin{equation}
    \m{A} = \m{L} \m{L}^T
\end{equation}

\subsubsection{Positive semidefinite matrices}

If a Hermitian matrix $ \m{A} $ is only positive semidefinite, instead of positive
definite, then it still has a decomposition of the form $ \m{A} = \m{L} \m{L}^* $
where the diagonal entries of $ \m{L} $ are allowed to be zero. The decompostion
need not be unique, for example:

\begin{equation}
    \begin{bmatrix}
        0 & 0 \\
        0 & 1\\
    \end{bmatrix}
    = \m{L} \m{L}^*
\end{equation}

\begin{equation}
    \m{L} =
    \begin{bmatrix}
        0 & 0 \\
        \cos \theta & \sin \theta
    \end{bmatrix}
\end{equation}

However, if the rank of $ \m{A} $ is $ r $, then there is a unique lower triangular
$ \m{L} $ with exactly $ r $ positive diagonal elements adn $ n - r $ columns
containing all zeros.

Alternatively, the decomposition can be made unique when  a pivoting choice is
fixed. Formally, if $ \m{A} $ is an $ n \times n $ positive semidefinite matrix
of rank $ r $, then there is at least one permutation matrix $ \m{P} $ such
that $ \m{P} \m{A} \m{P}^T $ has a unique decompostion of the form

\begin{equation}
    \m{P} \m{A} \m{P}^T = \m{L} \m{L}^*
\end{equation}

with

\begin{equation}
    \m{L} = \begin{bmatrix}
        \m{L}_1 & \m{0} \\
        \m{L}_2 & \m{0} \\
    \end{bmatrix}
\end{equation}

where $ \m{L}_1 $ is a $ r \times r $ lower triangular matrix with positive
diagonal.


\subsubsection{Example}

\begin{equation}
    \begin{bmatrix}
        \phantom{-1}4 & \phantom{-}12 & -16 \\
        \phantom{-}12 & \phantom{-}37 & -43 \\
        -16 & -43 & \phantom{-}98 \\
    \end{bmatrix}
    = \begin{bmatrix}
        \phantom{-}2 & 0 & 0 \\
        \phantom{-}6 & 1 & 0 \\
        -8 & 5 & 3 \\
    \end{bmatrix}
    \begin{bmatrix}
        2 & 6 & -8 \\
        0 & 1 & \phantom{-}5 \\
        0 & 0 & \phantom{-}3 \\
    \end{bmatrix}
\end{equation}


\subsubsection{Applications}

\textbf{Solution of a system of linear equations}

The Cholesky decomposition is mainly used for the numerical solution of
\textbf{linear equations} $ \m{A} \m{x} = \m{b} $. If $ \m{A} $ is symmetric
and positive definite, then we can solve $ \m{A} \m{x} = \m{b} $ by first computing
the Cholesky decomposition $ \m{A} = \m{L} \m{L}^* $, then solving
$ \m{L} \m{y} = \m{b} $ for $ \m{y} $ by forward substitution and finally solving
$ \m{L}^* \m{x} = \m{y} $ for $ \m{x} $ by back substitution.

\begin{eqarray}
    \m{A} \m{x} &= \m{b} \\
    \m{L} \m{L}^* \m{x} &= \m{b},\ substitute\ \m{L}^* \m{x} = \m{y} \\
    \m{L} \m{y} &= \m{b},\ then\ solve \\
    \m{L}^* \m{x} &= \m{y}
\end{eqarray}

For linear systems that can be put into symmetric form, the Cholesky decomposition
is the method choice, for superior efficiency and numerical stability. Compared
to the \textbf{LU} decomposition, it is roughly twice as efficient.

\textbf{Matrix Inversion}

A non-Hermitian matrix $ \m{B} $ can be inverted using the following identity,
where $ \m{B} \m{B}^* $ will always be Hermitian:

\begin{equation}
    \m{B}^{-1} = \m{B}^* ( \m{B} \m{B}^* )^{-1}
\end{equation}

The above stated matrix relation is also called the \index{pseudoinverse}
\textbf{pseudoinverse}
of a matrix. The following equality holds:

\begin{equation}
    \m{B}^* ( \m{B} \m{B}^* )^{-1} = ( \m{B}^* \m{B} )^{-1} \m{B}^*
\end{equation}

The best way to compute the \index{pseudoinverse} is to use
\index{singular value decomposition} \index{SVD}. With $ \m{A} $ being
$ \m{A} = \m{U} \m{S} \m{V}^* $,
where $ \m{U} $ and $ \m{V} $ (both $ n \times n $) are \index{orthogonal}
orthogonal and
$ \m{S} $ ( $ m  \times n $ ) is \index{diagonal} with real, non-negative
\textit{singular values} $ \sigma_{i,1} = 1, \dots, n $. We find that

\begin{equation}
    \m{A}^* = \m{V} ( \m{S}^* \m{S} )^{-1} \m{S}^* \m{U}^*
\end{equation}

If the rank $ r $ of $ \m{A} $ is smaller than $ n $, the inverse of
$ \m{S}^* \m{S} $ does not exist, and one uses only the first $ r $ singular
values. $ \m{S} $ then becomes an $ r \times r $ matrix and $ \m{U}$, $ \m{V} $
shrink accordingly.


\section{Linear Regression}
\subsection{Derivation}

\textbf{Linear Regression} is an approach for predicting a response using
a \textbf{single feature}. In linear regression we assume that two variables
i. e. independent and dependent are linearly related.

Generalrly we define $ \m{x} $ as a \textbf{feature vector}:

\begin{equation}
    \m{x} = \left[ x_1, x_2, ..., x_n \right]
\end{equation}

and $ \m{y} $ as \textbf{response vector}:

\begin{equation}
    \m{y} = \left[ y_1, y_2, ..., y_n \right]
\end{equation}

then a predicted response:

\begin{equation}
    h(x_i) = \beta_0 + \beta_1 \times x_i
\end{equation}

where: \\
$ x_i $ represents the \textit{i}-th observation \\
$ \beta_1 $ represents the slope, and \\
$ \beta_0 $ represents the intercept of the regression line.


To predict a new value $ y_i $, we need to consider:

\begin{equation}
    y_i = \beta_0 + \beta_1 \times x_i + \epsilon_i = h(x_i) + \epsilon_i
\end{equation}

where:\\
$ \epsilon_i $ is the \textbf{error} of the estimate.

The error function is therefore obtained:

\begin{equation}
    \epsilon_i = y_i - h(x_i)
\end{equation}

The $ \beta_0 $ and $ \beta_1 $ coefficients are obtained by minimizing the residual
\textbf{error} of the estimate using e.g. the \textbf{least square method}:

\begin{eqarray}
    J(\beta_0, \beta_1) &= \frac{1} {2 n} \sum_{i=1}^{n} \epsilon_i^2 \\
    &= \frac{1} {2 n} \sum_{i=1}^{n} \left( y_i - h(x_i) \right)^2 \\
    &= \frac{1} {2 n} \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 \times x_i  \right)^2
\end{eqarray}

To find the coefficients we can derviate the cost function, first with regards to $ \beta_0 $:

\begin{equation}
    \frac{\partial J} {\partial \beta_0} \left[ \frac{1} {2 n} \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 \times x_i  \right)^2 \right] = 0
\end{equation}

where using the chain rule we can write:

\begin{eqarray}
    u &= y_i - \beta_0 - \beta_1 \times x_i \\
    J &= \frac{1} {2 n} \sum_{i=1}^n u^2 \\
    \frac{\partial J} {\partial \beta_0} &= \frac{\partial J} {\partial u} \times \frac{\partial u} {\partial \beta_0} \\
    \frac{\partial J} {\partial u} &= 2 u \\
    \frac{\partial u} {\partial \beta_0} &= -1 \\
    \frac{\partial J} {\partial \beta_0} &= \frac{1} {2n} \times (-2)  \times \sum_{i=1}^n \left(y_i - \beta_0 - \beta_1 \times x_i \right) \\
    0 &= \frac{1} {n} \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 \times x_i \right) \\
    0 &= \frac{1} {n} \sum_{i=1}^n y_i - \frac{1} {n} \sum_{i=1}^n \beta_0 - \frac{1} {n} \beta_1 \sum_{i=1}^n x_i \\
    0 &= \bar{y}_i - \beta_0 - \beta_1 \bar{x}_i \\
    \beta_0 &= \bar{y} - \beta_1 \bar{x}
\end{eqarray}

To find $ \beta_1 $:

\begin{equation}
    \frac{\partial J} {\partial \beta_1} \left[ \frac{1} {2 n} \sum_{i=1}^{n} \left( y_i - \beta_0 - \beta_1 \times x_i  \right)^2 \right] = 0
\end{equation}

\begin{eqarray}
    u &= y_i - \beta_0 - \beta_1 \times x_i \\
    J &= \frac{1} {2 n} \sum_{i=1}^n u^2 \\
    \frac{\partial J} {\partial \beta_1} &= \frac{\partial J} {\partial u} \times \frac{\partial u} {\partial \beta_1} \\
    \frac{\partial J} {\partial u} &= 2 u \\
    \frac{\partial u} {\partial \beta_1} &= -x \\
    \frac{\partial J} {\partial \beta_1} &= \frac{1} {2 n} \times (-2) \sum_{i=1}^n x_i \times \left( y_i - \beta_0 - \beta_1 x_i \right) \\
    0 &= \frac{1}{n} \sum_{i=1}^n x_i \left( y_i - \beta_0 - \beta_1 x_i \right) \\
    0 &= \frac{1}{n} \sum_{i=1}^n \left( x_i y_i - \beta_0 x_i - \beta_1 x_i^2 \right)
\end{eqarray}

Substituting the value of $ \beta_0 $:

\begin{eqarray}
    \beta_0 &= \bar{y} - \beta_1 \bar{x} \\
    0 &= \frac{1} {n} \sum_{i=1}^n \left(x_i y_i - \left(\bar{y} - \beta_1 \bar{x} \right) x_i - \beta_1 x_i^2 \right) \\
    0 &= \frac{1} {n} \sum_{i=1}^n \left(x_i y_i - \bar{y} x_i + \beta_1 \bar{x} x_i - \beta_1 x_i^2 \right) \\
    0 &= \frac{1} {n} \left( \sum_{i=1}^n \left(x_i y_i - \bar{y} x_i \right) - \beta_1 \sum_{i=1}^n \left(x_i^2 - \bar{x} x_i \right) \right) \\
    \beta_1 &= \frac{\frac{1}{n} \sum_{i=1}^n \left(x_i y_i - \bar{y} x_i \right)}
                    {\frac{1}{n} \sum_{i=1}^n \left(x_i^2 - \bar{x} x_i \right)}
\end{eqarray}

Simplifying the formula using:

\begin{eqarray}
    \bar{y} &= \frac{1}{n} \sum_{i=1}^n y_i \\
    \bar{x} &= \frac{1}{n} \sum_{i=1}^n x_i
\end{eqarray}

we get:
\begin{eqarray}
    \beta_1 &= \frac{\frac{1}{n} \sum_{i=1}^n x_i y_i - \bar{x} \bar{y}}
                    {\frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x} \bar{x}} \\
    \beta_1 &= \frac{\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}}
                    {\sum_{i=1}^n x_i^2 - n \bar{x}^2}
\end{eqarray}


\subsection{Result}

\begin{bbox}
    So, to recapitulate, to get a linear regression coefficients for a dataset:

    \begin{eqarray}
        \m{x} &= \left[ x_1, x_2, ..., x_n \right] \\
        \m{y} &= \left[ y_1, y_2, ..., y_n \right]
    \end{eqarray}

    we may use equations:

    \begin{eqarray}
        \beta_1 &= \frac{\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}}
                        {\sum_{i=1}^n x_i^2 - n \bar{x}^2} \\
        \beta_0 &= \bar{y} - \beta_1 \bar{x}
    \end{eqarray}

    with an error:
    \begin{equation}
        \epsilon_i = y_i - \beta_0 - \beta_1 x_i
    \end{equation}

\end{bbox}



